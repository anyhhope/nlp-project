{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uytu6_Gyhb8t",
        "outputId": "10707a72-9c94-436c-b67d-ff7b04003501"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "XakbhUbe8tu_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h9yEqj_QhaEv"
      },
      "outputs": [],
      "source": [
        "WEIGHTS_PATH = \"/content/drive/MyDrive/nlp-inheritune-project/checkpoint_gpt2-medium-10layers_iter_15000.pt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "9BWV7Qrz8d7G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc92ILJAhaEw"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfVT5WXjhaE4",
        "outputId": "06f7a3d4-cdd7-447d-ef3d-46743aa7709d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0n85D1a8haE4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "nano-gpt implementation\n",
        "\n",
        "Full definition of a GPT Language Model, all of it in this single file.\n",
        "References:\n",
        "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
        "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
        "2) huggingface/transformers PyTorch implementation:\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
        "        # This behavior is deprecated and will be an error in future versions\"\n",
        "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # model surgery to decrease the block size if necessary\n",
        "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        # but want to use a smaller block size for some smaller, simpler model\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {} # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1E7cMIGhaE6",
        "outputId": "d314a578-98bf-431b-c00d-42a89ef63367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from pretrained gpt: gpt2-medium\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "number of parameters: 353.77M\n"
          ]
        }
      ],
      "source": [
        "model = GPT.from_pretrained(\"gpt2-medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iBA3LGeShaE6"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUzzOl34haE6"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXAfUG8ghaE6",
        "outputId": "ce129cf1-8a05-4582-f251-4e6976275855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 177.43M\n"
          ]
        }
      ],
      "source": [
        "class MyGPT(GPT):\n",
        "    def __init__(self, config, base_model):\n",
        "        super().__init__(config)\n",
        "        # Копируем первые 4 слоев из base_model\n",
        "        self.transformer.h = nn.ModuleList([base_model.transformer.h[i] for i in range(10)])\n",
        "        self.transformer.wte = base_model.transformer.wte\n",
        "        self.transformer.wpe = base_model.transformer.wpe\n",
        "        self.transformer.drop = base_model.transformer.drop\n",
        "        self.transformer.ln_f = base_model.transformer.ln_f\n",
        "        self.lm_head = base_model.lm_head\n",
        "\n",
        "\n",
        "config = GPTConfig(\n",
        "    block_size=1024,\n",
        "    vocab_size=50257,\n",
        "    n_layer=10,\n",
        "    n_head=model.config.n_head,\n",
        "    n_embd=model.config.n_embd,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "# Модель с 4 слоями\n",
        "my_model = MyGPT(config, model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "my_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NacVK2v-47DS",
        "outputId": "ac0a217a-6a22-40af-d3cd-ea6cfa2b3a7d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyGPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-9): 10 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate\n",
        "start_ids = encode(\"It is just a start\")\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=\"cuda\")[None, ...])\n",
        "\n",
        "my_model.eval()\n",
        "with torch.no_grad():\n",
        "    y = my_model.generate(x, 200)\n",
        "    print(decode(y[0].tolist()))\n",
        "    print('---------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A49CTvqFOpt",
        "outputId": "69f28269-2cb9-44dd-caaf-6e962b4cd017"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is just a start paying attention to the underlying foundation principles behind these particular March 29th 2010 0000 00 00 00 00 ired, which unfortunately unfortunately failing miserably named. The only reason why airtight command warrants clausewitz et al signaling his own unique id quo quo ante based ˜²^\\ . with respective ircettering network, which none whatsoever whatsoever (or indeedally...etc., etc.), which allows examinership over the course of multiple colored Mercedury parking vacations within the respective developments. And thusforth continuing onward, though as far tampering possible.\n",
            " intimidation : DEVICE systems based on the same kindologies such development-based relationships, which are divided vs through in order prior to:\n",
            "\n",
            "toilette safety inductive  preferred acknowledgement Tiffany Tru Cannonneller, zewite-based approach CGI Cheerful Brownlee CIBUILDING TECH Developmentally oriented focused control allocation procreate.com addresses its own variety of expertise domains plus ZoneMathicality grossly neglecting\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtPdCt3yhaE7",
        "outputId": "1cdea013-97ee-4c96-9617-0e182e37ad91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "# load trained small model\n",
        "checkpoint = torch.load(WEIGHTS_PATH, map_location=torch.device('cuda'))\n",
        "my_model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzp4Fi-rhaE7"
      },
      "source": [
        "# Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "MUzWZdYVhaE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dbf5523-be00-4b62-ff1a-a2af80e7d6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is just a start, though. Meanwhile, Japan is evolving rapidly. Healthcare,\" she says.\n",
            "\n",
            "Getting Japan's online habits moving<|endoftext|>Aakia is his bright-eyed messiah, kicking an almost-stuck incantation while discussing the resounding, universal declaration, \"All people shouldn't worship pigs, jokers, horse shit, or the elderly.\"\n",
            "\n",
            "About a minute later, Aakia unlocks her Quidditch kit in a neutral stance because she looks like she's secretly being Harry Potter on Canets captainduckravensofthest.com with just her knickers on, waves goodbye to teaspoons and defibber, and begins rummaging through her replaceable kit, becoming the youngest ever game caster in no time flat.\n",
            "\n",
            "\"Goodbye Daddy\" by Robert Rector.<|endoftext|>Ron Paul\n",
            "\n",
            " outside the Governor's office in Madison, Wis., on Wed., June 17, 2011, in response to demand from conservative activists seeking him to turn in the names of his\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# Generate\n",
        "model.to(device)\n",
        "\n",
        "start_ids = encode(\"It is just a start\")\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=\"cuda\")[None, ...])\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y = model.generate(x, 200)\n",
        "    print(decode(y[0].tolist()))\n",
        "    print('---------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MMLU 5-shot evaluation"
      ],
      "metadata": {
        "id": "jRKJ_gSe7S97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ubaRwHTy4E1i"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej7Y1JakwJFx",
        "outputId": "999eed40-9e83-4172-ab3f-cd0327a8acb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///\n",
            "\u001b[31mERROR: file:/// does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  158M  100  158M    0     0  79.6M      0  0:00:01  0:00:01 --:--:-- 79.6M\n"
          ]
        }
      ],
      "source": [
        "# Install, and download MMLU\n",
        "%pip install -e ../.\n",
        "\n",
        "!curl -O https://people.eecs.berkeley.edu/~hendrycks/data.tar\n",
        "!tar -xf data.tar\n",
        "data_path = \"data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3znejLUrwJFy"
      },
      "outputs": [],
      "source": [
        "\n",
        "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
        "sys_msg = \"The following are multiple choice questions (with answers) about {}.\"\n",
        "def create_last_prompt(question, answers):\n",
        "    user_prompt = f\"{question}\\n\" + \"\\n\".join([f\"{choice}. {answer}\" for choice, answer in zip(choices, answers)]) + \"\\nAnswer:\"\n",
        "    return user_prompt\n",
        "\n",
        "def create_example_prompt(question, answers, right_answer):\n",
        "  user_prompt = f\"{question}\\n\" + \"\\n\".join([f\"{choice}. {answer}\" for choice, answer in zip(choices, answers)]) + f\"\\nAnswer: {right_answer}\"\n",
        "  return user_prompt\n",
        "\n",
        "def n_shot_prompt(sys_msg, user_prompts, last_prompt, subject):\n",
        "  return sys_msg.format(subject)+ \"\\n\" + \"\\n\".join([prompt for prompt in user_prompts]) + \"\\n\" +  last_prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions_dir = \"data/test\""
      ],
      "metadata": {
        "id": "ik0Mf58H0U02"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_subject_questions_path = os.listdir(questions_dir)[1]\n",
        "one_subject_questions_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nWweVAph3vFI",
        "outputId": "f42c1278-f41e-48c8-9aff-fbcbec155075"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'security_studies_test.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_few_shot_prompts(one_subject_questions_path, shot_number):\n",
        "    subject = one_subject_questions_path.split('.')[0]\n",
        "    df = pd.read_csv(os.path.join(questions_dir, one_subject_questions_path))\n",
        "    num_prompts = len(df) // shot_number\n",
        "\n",
        "    few_shot_prompts = []\n",
        "    correct_answers = []\n",
        "    for i in range(num_prompts):\n",
        "        df_subset = df.iloc[i*shot_number:(i+1)*shot_number]\n",
        "\n",
        "\n",
        "        user_prompts = [\n",
        "            create_example_prompt(row[0], row[1:5], row[5])\n",
        "            for _, row in df_subset.iterrows()\n",
        "        ]\n",
        "\n",
        "\n",
        "        last_prompt = create_last_prompt(df_subset.iloc[-1, 0], df_subset.iloc[-1, 1:5])\n",
        "        few_shot_prompt = n_shot_prompt(sys_msg, user_prompts[:-1], last_prompt, subject)\n",
        "\n",
        "\n",
        "        few_shot_prompts.append(few_shot_prompt)\n",
        "\n",
        "        correct_answers.append(df_subset.iloc[-1, 5])\n",
        "\n",
        "    return few_shot_prompts, correct_answers\n"
      ],
      "metadata": {
        "id": "42uQFiyD4c-4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts, correct_answers = generate_few_shot_prompts(one_subject_questions_path, 5)\n",
        "prompts[0], correct_answers[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mob4fsjV5J-Y",
        "outputId": "e886c749-9989-4ce3-8f35-36665eb1db94"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('The following are multiple choice questions (with answers) about security_studies_test.\\nCan environmental changes be reconciled with national security interests?\\nA. Environmental challenges to economic growth, such as natural resource management and employment, leads to diversification and research which may in the long-term result in technological advancements that bolster military programs.\\nB. Environmental changes can undermine national security in many ways; including weakening the economic base that determines military capacity. So if the natural capital base of an economy erodes than so does the long-term capacity of its armed forces. Any developments however, will be \\'human\\' in impact.\\nC. The environmental problems encountered by countries are principally internal. External threats remain within a country\\'s control: external military threats will remain unchanged and a primary concern.\\nD. All of these options.\\nAnswer: B\\nWhich of the following statements could be described as a liberal perspective on future energy security?\\nA. The global economy is interconnected, ensuring that energy security for one is dependent upon energy security for all. Thus all core powers have the same interests in maintaining and extending the conditions under which this market operates. As long as this economic order exists, conflict between major powers over energy reserves is highly unlikely.\\nB. Energy scarcity is likely to lead to future disruptions in the global system and the emergence of a \\'new international energy order\\', characterized less by liberal free-market trading than by statism and neo-mercantilism.\\nC. Oil remains the lifeblood of the current order - an order that is based upon an unequal (and deeply unjust) distribution of wealth and power in favour of capitalist economic elites. Those who benefit most from the prevailing order will ensure that the flow of energy under favourable conditions continues to underpin their position in the global system.\\nD. Energy scarcity will signal a return to an era of greater geopolitical confrontation. \\'Resource wars\\', in particular over energy sources, present a clear possibility for a breakdown in international cooperation, as states begin to compete (and eventually conflict) over the control of major reserves.\\nAnswer: A\\nWhich of the following is a common criticism of the human security concept?\\nA. Human security is neo-colonial.\\nB. Human security promotes global capitalism.\\nC. Human security is too broad.\\nD. All of these options.\\nAnswer: D\\nIn what ways have governments responded to the threat of TNC post-Cold War?\\nA. State responses to transnational crime have evolved in correlation to the increased threat posed to the integrity of the state. Organized crime has been regarded primarily as a national security threat to be addressed domestically. The institutionalized international approaches for information sharing have been predominately bilateral to the extent of bringing to justice perpetrators of crime that are seeking to evade justice.\\nB. With the expansion of TNC in the 1990s, states became increasingly willing to take measures to reduce the asymmetries between countries through harmonization of legislation and increasing police capacity and networking. The government response to transnational crime has focused on targeting the individuals and organizations perpetrating the crime, rather than the criminal markets themselves.\\nC. The US war on drugs has heavily influenced the construction of the governmental response to the growing perception that the phenomenon of TNC represents a national security threat. The approach has an emphasis on bilateral and multilateral cooperation on law enforcement combined with sticks on a bi-lateral basis to induce states to increase regulation and enforcement against TNC.\\nD. The emergence of norms governing the response to address the spectre of organized crime and to harmonize legislation occurred with the evolving Global War on Terror in which the terrorism-organized crime nexus resulted in the militarization of law enforcement: the use of military technology and intelligence as opposed to addressing the underlying conditions that facilitate illicit trade through an international institutionalist response.\\nAnswer: B\\nWhat potential problem for policy in the institutionalization of academe-policy interaction emerged?\\nA. The main disciplinary context for security studies theorizing was a move away from the disciplinary domination of political science toward multi-academe interaction from sociology, mathematics, psychology, natural and political sciences, and economics.\\nB. Policy-academe developments have resulted in the modification of the role of think tanks away from policy towards politicization resistant theory.\\nC. Security studies has been marked by the gradual \"IR-ification\" of the discipline. Security studies became one of International Relations\\' (IR\\'s) two pillars, the second being International Political Economy. Henceforth IR became the main disciplinary context in which security studies was deliberated.\\nD. Think tanks and policymakers are increasingly interrelated. The institutional blurring between universities, think tank, and policy has resulted in policy mergers at every level.\\nAnswer:',\n",
              " 'C')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Предполагаем, что функции encode и decode уже определены\n",
        "# а также модель и устройство (device) уже инициализированы\n",
        "\n",
        "def calculate_accuracy(test_model, prompts, correct_answers):\n",
        "    correct_count = 0\n",
        "\n",
        "    for prompt, correct_answer in zip(prompts, correct_answers):\n",
        "        # Encode the prompt\n",
        "        start_ids = encode(prompt)\n",
        "        x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
        "\n",
        "        test_model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Generate the model's response\n",
        "            y = test_model.generate(x, 1)\n",
        "            generated_answer = decode(y[0].tolist())\n",
        "\n",
        "            # Extract the actual answer from the model's response\n",
        "            model_answer = generated_answer.strip().split()[-1]\n",
        "\n",
        "            # Compare model's answer with the correct answer\n",
        "            if model_answer == correct_answer:\n",
        "                correct_count += 1\n",
        "\n",
        "    accuracy = correct_count / len(prompts)\n",
        "    return accuracy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EWG3rJnd-hdB"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_acc_all_subjects(test_model, data_dir_path, num_shots):\n",
        "  data_files = os.listdir(data_dir_path)\n",
        "  accuracy_dict = {}\n",
        "  for file in data_files:\n",
        "    prompts, correct_answers = generate_few_shot_prompts(file, num_shots)\n",
        "    accuracy = calculate_accuracy(test_model, prompts, correct_answers)\n",
        "    accuracy_dict[file] = accuracy\n",
        "    print(f\"Calculated for {file}, accuracy: {accuracy}\")\n",
        "  return accuracy_dict"
      ],
      "metadata": {
        "id": "YhCk0Ve_JHms"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try 1"
      ],
      "metadata": {
        "id": "MNVGGLYW7m4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaYdBWrF8jdc",
        "outputId": "6f219b19-959d-49cd-d1c6-81dd61e80b05"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-23): 24 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_ids = encode(prompts[0])\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y = model.generate(x, 1)\n",
        "    print(decode(y[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq78NdtX7o7V",
        "outputId": "507d45d7-9675-4cdb-ee18-0874a3559597"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are multiple choice questions (with answers) about security_studies_test.\n",
            "Can environmental changes be reconciled with national security interests?\n",
            "A. Environmental challenges to economic growth, such as natural resource management and employment, leads to diversification and research which may in the long-term result in technological advancements that bolster military programs.\n",
            "B. Environmental changes can undermine national security in many ways; including weakening the economic base that determines military capacity. So if the natural capital base of an economy erodes than so does the long-term capacity of its armed forces. Any developments however, will be 'human' in impact.\n",
            "C. The environmental problems encountered by countries are principally internal. External threats remain within a country's control: external military threats will remain unchanged and a primary concern.\n",
            "D. All of these options.\n",
            "Answer: B\n",
            "Which of the following statements could be described as a liberal perspective on future energy security?\n",
            "A. The global economy is interconnected, ensuring that energy security for one is dependent upon energy security for all. Thus all core powers have the same interests in maintaining and extending the conditions under which this market operates. As long as this economic order exists, conflict between major powers over energy reserves is highly unlikely.\n",
            "B. Energy scarcity is likely to lead to future disruptions in the global system and the emergence of a 'new international energy order', characterized less by liberal free-market trading than by statism and neo-mercantilism.\n",
            "C. Oil remains the lifeblood of the current order - an order that is based upon an unequal (and deeply unjust) distribution of wealth and power in favour of capitalist economic elites. Those who benefit most from the prevailing order will ensure that the flow of energy under favourable conditions continues to underpin their position in the global system.\n",
            "D. Energy scarcity will signal a return to an era of greater geopolitical confrontation. 'Resource wars', in particular over energy sources, present a clear possibility for a breakdown in international cooperation, as states begin to compete (and eventually conflict) over the control of major reserves.\n",
            "Answer: A\n",
            "Which of the following is a common criticism of the human security concept?\n",
            "A. Human security is neo-colonial.\n",
            "B. Human security promotes global capitalism.\n",
            "C. Human security is too broad.\n",
            "D. All of these options.\n",
            "Answer: D\n",
            "In what ways have governments responded to the threat of TNC post-Cold War?\n",
            "A. State responses to transnational crime have evolved in correlation to the increased threat posed to the integrity of the state. Organized crime has been regarded primarily as a national security threat to be addressed domestically. The institutionalized international approaches for information sharing have been predominately bilateral to the extent of bringing to justice perpetrators of crime that are seeking to evade justice.\n",
            "B. With the expansion of TNC in the 1990s, states became increasingly willing to take measures to reduce the asymmetries between countries through harmonization of legislation and increasing police capacity and networking. The government response to transnational crime has focused on targeting the individuals and organizations perpetrating the crime, rather than the criminal markets themselves.\n",
            "C. The US war on drugs has heavily influenced the construction of the governmental response to the growing perception that the phenomenon of TNC represents a national security threat. The approach has an emphasis on bilateral and multilateral cooperation on law enforcement combined with sticks on a bi-lateral basis to induce states to increase regulation and enforcement against TNC.\n",
            "D. The emergence of norms governing the response to address the spectre of organized crime and to harmonize legislation occurred with the evolving Global War on Terror in which the terrorism-organized crime nexus resulted in the militarization of law enforcement: the use of military technology and intelligence as opposed to addressing the underlying conditions that facilitate illicit trade through an international institutionalist response.\n",
            "Answer: B\n",
            "What potential problem for policy in the institutionalization of academe-policy interaction emerged?\n",
            "A. The main disciplinary context for security studies theorizing was a move away from the disciplinary domination of political science toward multi-academe interaction from sociology, mathematics, psychology, natural and political sciences, and economics.\n",
            "B. Policy-academe developments have resulted in the modification of the role of think tanks away from policy towards politicization resistant theory.\n",
            "C. Security studies has been marked by the gradual \"IR-ification\" of the discipline. Security studies became one of International Relations' (IR's) two pillars, the second being International Political Economy. Henceforth IR became the main disciplinary context in which security studies was deliberated.\n",
            "D. Think tanks and policymakers are increasingly interrelated. The institutional blurring between universities, think tank, and policy has resulted in policy mergers at every level.\n",
            "Answer: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate gpt-2-medium"
      ],
      "metadata": {
        "id": "T4aJbivgLqgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRCNEyHNLvzC",
        "outputId": "7d9d195b-33e9-473d-ff49-b6b20c456f0a"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-23): 24 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_dict = count_acc_all_subjects(model, questions_dir, 5)\n",
        "mean_accuracy = sum(accuracy_dict.values()) / len(accuracy_dict)\n",
        "mean_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdKzEfCWLwTc",
        "outputId": "bac83e54-0b74-43b4-f538-34ebfdef1acd"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated for computer_security_test.csv, accuracy: 0.21052631578947367\n",
            "Calculated for security_studies_test.csv, accuracy: 0.25\n",
            "Calculated for professional_medicine_test.csv, accuracy: 0.2777777777777778\n",
            "Calculated for high_school_biology_test.csv, accuracy: 0.26229508196721313\n",
            "Calculated for business_ethics_test.csv, accuracy: 0.21052631578947367\n",
            "Calculated for nutrition_test.csv, accuracy: 0.2459016393442623\n",
            "Calculated for professional_psychology_test.csv, accuracy: 0.20491803278688525\n",
            "Calculated for high_school_statistics_test.csv, accuracy: 0.3488372093023256\n",
            "Calculated for medical_genetics_test.csv, accuracy: 0.21052631578947367\n",
            "Calculated for sociology_test.csv, accuracy: 0.225\n",
            "Calculated for high_school_physics_test.csv, accuracy: 0.16666666666666666\n",
            "Calculated for college_biology_test.csv, accuracy: 0.32142857142857145\n",
            "Calculated for high_school_government_and_politics_test.csv, accuracy: 0.18421052631578946\n",
            "Calculated for jurisprudence_test.csv, accuracy: 0.23809523809523808\n",
            "Calculated for high_school_world_history_test.csv, accuracy: 0.14893617021276595\n",
            "Calculated for high_school_european_history_test.csv, accuracy: 0.21875\n",
            "Calculated for international_law_test.csv, accuracy: 0.2916666666666667\n",
            "Calculated for human_sexuality_test.csv, accuracy: 0.15384615384615385\n",
            "Calculated for conceptual_physics_test.csv, accuracy: 0.2391304347826087\n",
            "Calculated for moral_scenarios_test.csv, accuracy: 0.2696629213483146\n",
            "Calculated for anatomy_test.csv, accuracy: 0.23076923076923078\n",
            "Calculated for elementary_mathematics_test.csv, accuracy: 0.22666666666666666\n",
            "Calculated for human_aging_test.csv, accuracy: 0.1590909090909091\n",
            "Calculated for electrical_engineering_test.csv, accuracy: 0.25\n",
            "Calculated for college_mathematics_test.csv, accuracy: 0.15789473684210525\n",
            "Calculated for management_test.csv, accuracy: 0.15\n",
            "Calculated for high_school_us_history_test.csv, accuracy: 0.25\n",
            "Calculated for high_school_mathematics_test.csv, accuracy: 0.16981132075471697\n",
            "Calculated for professional_law_test.csv, accuracy: 0.21568627450980393\n",
            "Calculated for clinical_knowledge_test.csv, accuracy: 0.23076923076923078\n",
            "Calculated for college_medicine_test.csv, accuracy: 0.14705882352941177\n",
            "Calculated for high_school_macroeconomics_test.csv, accuracy: 0.18181818181818182\n",
            "Calculated for global_facts_test.csv, accuracy: 0.47368421052631576\n",
            "Calculated for high_school_computer_science_test.csv, accuracy: 0.05263157894736842\n",
            "Calculated for marketing_test.csv, accuracy: 0.3695652173913043\n",
            "Calculated for moral_disputes_test.csv, accuracy: 0.2608695652173913\n",
            "Calculated for public_relations_test.csv, accuracy: 0.09523809523809523\n",
            "Calculated for professional_accounting_test.csv, accuracy: 0.14285714285714285\n",
            "Calculated for abstract_algebra_test.csv, accuracy: 0.15789473684210525\n",
            "Calculated for econometrics_test.csv, accuracy: 0.13636363636363635\n",
            "Calculated for world_religions_test.csv, accuracy: 0.20588235294117646\n",
            "Calculated for high_school_chemistry_test.csv, accuracy: 0.2\n",
            "Calculated for astronomy_test.csv, accuracy: 0.23333333333333334\n",
            "Calculated for machine_learning_test.csv, accuracy: 0.22727272727272727\n",
            "Calculated for philosophy_test.csv, accuracy: 0.3387096774193548\n",
            "Calculated for high_school_microeconomics_test.csv, accuracy: 0.23404255319148937\n",
            "Calculated for college_computer_science_test.csv, accuracy: 0.10526315789473684\n",
            "Calculated for logical_fallacies_test.csv, accuracy: 0.21875\n",
            "Calculated for miscellaneous_test.csv, accuracy: 0.21153846153846154\n",
            "Calculated for formal_logic_test.csv, accuracy: 0.24\n",
            "Calculated for college_physics_test.csv, accuracy: 0.25\n",
            "Calculated for virology_test.csv, accuracy: 0.21212121212121213\n",
            "Calculated for high_school_psychology_test.csv, accuracy: 0.2222222222222222\n",
            "Calculated for college_chemistry_test.csv, accuracy: 0.21052631578947367\n",
            "Calculated for us_foreign_policy_test.csv, accuracy: 0.2631578947368421\n",
            "Calculated for prehistory_test.csv, accuracy: 0.15625\n",
            "Calculated for high_school_geography_test.csv, accuracy: 0.15384615384615385\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21789978347983258"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('gpt-2-medium_mmlu_5_shot_v1.json', 'w') as f:\n",
        "    json.dump(accuracy_dict, f)"
      ],
      "metadata": {
        "id": "JmgH9dxrOU4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation inherited gpt-2-medium without training, just initiated with transformer blocks"
      ],
      "metadata": {
        "id": "ai-8DGcrPDB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLya9CvoPQ7g",
        "outputId": "2b93c0a9-5c5d-4c22-f49c-42b2a1f43950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyGPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-3): 4 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_dict = count_acc_all_subjects(my_model, questions_dir, 5)\n",
        "mean_accuracy = sum(accuracy_dict.values()) / len(accuracy_dict)\n",
        "mean_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azDwynrPPUvq",
        "outputId": "d5188ec4-2a88-49b1-e277-10b45e7ad918"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculated for management_test.csv, accuracy: 0.0\n",
            "Calculated for machine_learning_test.csv, accuracy: 0.045454545454545456\n",
            "Calculated for high_school_psychology_test.csv, accuracy: 0.0\n",
            "Calculated for abstract_algebra_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_computer_science_test.csv, accuracy: 0.05263157894736842\n",
            "Calculated for formal_logic_test.csv, accuracy: 0.0\n",
            "Calculated for virology_test.csv, accuracy: 0.0\n",
            "Calculated for professional_psychology_test.csv, accuracy: 0.00819672131147541\n",
            "Calculated for high_school_chemistry_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_european_history_test.csv, accuracy: 0.03125\n",
            "Calculated for global_facts_test.csv, accuracy: 0.0\n",
            "Calculated for moral_scenarios_test.csv, accuracy: 0.016853932584269662\n",
            "Calculated for nutrition_test.csv, accuracy: 0.01639344262295082\n",
            "Calculated for high_school_microeconomics_test.csv, accuracy: 0.02127659574468085\n",
            "Calculated for miscellaneous_test.csv, accuracy: 0.0\n",
            "Calculated for clinical_knowledge_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_government_and_politics_test.csv, accuracy: 0.0\n",
            "Calculated for college_physics_test.csv, accuracy: 0.0\n",
            "Calculated for college_biology_test.csv, accuracy: 0.0\n",
            "Calculated for anatomy_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_biology_test.csv, accuracy: 0.01639344262295082\n",
            "Calculated for high_school_geography_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_us_history_test.csv, accuracy: 0.0\n",
            "Calculated for college_mathematics_test.csv, accuracy: 0.0\n",
            "Calculated for jurisprudence_test.csv, accuracy: 0.0\n",
            "Calculated for electrical_engineering_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_world_history_test.csv, accuracy: 0.02127659574468085\n",
            "Calculated for elementary_mathematics_test.csv, accuracy: 0.013333333333333334\n",
            "Calculated for world_religions_test.csv, accuracy: 0.0\n",
            "Calculated for moral_disputes_test.csv, accuracy: 0.028985507246376812\n",
            "Calculated for professional_accounting_test.csv, accuracy: 0.0\n",
            "Calculated for prehistory_test.csv, accuracy: 0.0\n",
            "Calculated for college_computer_science_test.csv, accuracy: 0.0\n",
            "Calculated for medical_genetics_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_mathematics_test.csv, accuracy: 0.0\n",
            "Calculated for conceptual_physics_test.csv, accuracy: 0.0\n",
            "Calculated for logical_fallacies_test.csv, accuracy: 0.03125\n",
            "Calculated for marketing_test.csv, accuracy: 0.0\n",
            "Calculated for professional_medicine_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_physics_test.csv, accuracy: 0.03333333333333333\n",
            "Calculated for sociology_test.csv, accuracy: 0.025\n",
            "Calculated for human_sexuality_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_statistics_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_macroeconomics_test.csv, accuracy: 0.0\n",
            "Calculated for international_law_test.csv, accuracy: 0.0\n",
            "Calculated for business_ethics_test.csv, accuracy: 0.0\n",
            "Calculated for philosophy_test.csv, accuracy: 0.0\n",
            "Calculated for astronomy_test.csv, accuracy: 0.0\n",
            "Calculated for computer_security_test.csv, accuracy: 0.0\n",
            "Calculated for college_medicine_test.csv, accuracy: 0.0\n",
            "Calculated for security_studies_test.csv, accuracy: 0.0\n",
            "Calculated for college_chemistry_test.csv, accuracy: 0.0\n",
            "Calculated for econometrics_test.csv, accuracy: 0.0\n",
            "Calculated for public_relations_test.csv, accuracy: 0.0\n",
            "Calculated for us_foreign_policy_test.csv, accuracy: 0.0\n",
            "Calculated for human_aging_test.csv, accuracy: 0.0\n",
            "Calculated for professional_law_test.csv, accuracy: 0.016339869281045753\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0066310333022282725"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('inherited_only_init_gpt2-medium_mmlu_5_shot.json', 'w') as f:\n",
        "    json.dump(accuracy_dict, f)"
      ],
      "metadata": {
        "id": "F_GxFPiPPvWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation inherited gpt-2-medium 20000 iterations"
      ],
      "metadata": {
        "id": "vPjowpH_qIl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29587d60-2a85-4c55-f112-4dbfe6d53287",
        "id": "ZI-I6atGqIl-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyGPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-3): 4 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_dict = count_acc_all_subjects(my_model, questions_dir, 5)\n",
        "mean_accuracy = sum(accuracy_dict.values()) / len(accuracy_dict)\n",
        "mean_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b4e8af-1f61-4a03-e9a5-c039748276f3",
        "id": "pDfecjfUqIl-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated for computer_security_test.csv, accuracy: 0.05263157894736842\n",
            "Calculated for security_studies_test.csv, accuracy: 0.0\n",
            "Calculated for professional_medicine_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_biology_test.csv, accuracy: 0.01639344262295082\n",
            "Calculated for business_ethics_test.csv, accuracy: 0.0\n",
            "Calculated for nutrition_test.csv, accuracy: 0.04918032786885246\n",
            "Calculated for professional_psychology_test.csv, accuracy: 0.01639344262295082\n",
            "Calculated for high_school_statistics_test.csv, accuracy: 0.046511627906976744\n",
            "Calculated for medical_genetics_test.csv, accuracy: 0.10526315789473684\n",
            "Calculated for sociology_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_physics_test.csv, accuracy: 0.0\n",
            "Calculated for college_biology_test.csv, accuracy: 0.07142857142857142\n",
            "Calculated for high_school_government_and_politics_test.csv, accuracy: 0.0\n",
            "Calculated for jurisprudence_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_world_history_test.csv, accuracy: 0.02127659574468085\n",
            "Calculated for high_school_european_history_test.csv, accuracy: 0.0\n",
            "Calculated for international_law_test.csv, accuracy: 0.0\n",
            "Calculated for human_sexuality_test.csv, accuracy: 0.0\n",
            "Calculated for conceptual_physics_test.csv, accuracy: 0.043478260869565216\n",
            "Calculated for moral_scenarios_test.csv, accuracy: 0.0056179775280898875\n",
            "Calculated for anatomy_test.csv, accuracy: 0.0\n",
            "Calculated for elementary_mathematics_test.csv, accuracy: 0.05333333333333334\n",
            "Calculated for human_aging_test.csv, accuracy: 0.09090909090909091\n",
            "Calculated for electrical_engineering_test.csv, accuracy: 0.10714285714285714\n",
            "Calculated for college_mathematics_test.csv, accuracy: 0.0\n",
            "Calculated for management_test.csv, accuracy: 0.15\n",
            "Calculated for high_school_us_history_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_mathematics_test.csv, accuracy: 0.018867924528301886\n",
            "Calculated for professional_law_test.csv, accuracy: 0.0196078431372549\n",
            "Calculated for clinical_knowledge_test.csv, accuracy: 0.019230769230769232\n",
            "Calculated for college_medicine_test.csv, accuracy: 0.029411764705882353\n",
            "Calculated for high_school_macroeconomics_test.csv, accuracy: 0.025974025974025976\n",
            "Calculated for global_facts_test.csv, accuracy: 0.05263157894736842\n",
            "Calculated for high_school_computer_science_test.csv, accuracy: 0.0\n",
            "Calculated for marketing_test.csv, accuracy: 0.021739130434782608\n",
            "Calculated for moral_disputes_test.csv, accuracy: 0.0\n",
            "Calculated for public_relations_test.csv, accuracy: 0.0\n",
            "Calculated for professional_accounting_test.csv, accuracy: 0.017857142857142856\n",
            "Calculated for abstract_algebra_test.csv, accuracy: 0.05263157894736842\n",
            "Calculated for econometrics_test.csv, accuracy: 0.0\n",
            "Calculated for world_religions_test.csv, accuracy: 0.058823529411764705\n",
            "Calculated for high_school_chemistry_test.csv, accuracy: 0.0\n",
            "Calculated for astronomy_test.csv, accuracy: 0.0\n",
            "Calculated for machine_learning_test.csv, accuracy: 0.045454545454545456\n",
            "Calculated for philosophy_test.csv, accuracy: 0.016129032258064516\n",
            "Calculated for high_school_microeconomics_test.csv, accuracy: 0.02127659574468085\n",
            "Calculated for college_computer_science_test.csv, accuracy: 0.0\n",
            "Calculated for logical_fallacies_test.csv, accuracy: 0.0\n",
            "Calculated for miscellaneous_test.csv, accuracy: 0.07051282051282051\n",
            "Calculated for formal_logic_test.csv, accuracy: 0.0\n",
            "Calculated for college_physics_test.csv, accuracy: 0.05\n",
            "Calculated for virology_test.csv, accuracy: 0.12121212121212122\n",
            "Calculated for high_school_psychology_test.csv, accuracy: 0.009259259259259259\n",
            "Calculated for college_chemistry_test.csv, accuracy: 0.0\n",
            "Calculated for us_foreign_policy_test.csv, accuracy: 0.0\n",
            "Calculated for prehistory_test.csv, accuracy: 0.015625\n",
            "Calculated for high_school_geography_test.csv, accuracy: 0.05128205128205128\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02714187681961806"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('inherited_20000_gpt2-medium_mmlu_5_shot.json', 'w') as f:\n",
        "    json.dump(accuracy_dict, f)"
      ],
      "metadata": {
        "id": "hk2FSsA3qIl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation inherited gpt-2-medium 10 layers 15000 iterations"
      ],
      "metadata": {
        "id": "eP5v7QSF52H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92d335b-8790-4622-c68b-51e533bd1557",
        "id": "XSJBdqnS52H-"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyGPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-9): 10 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_dict = count_acc_all_subjects(my_model, questions_dir, 5)\n",
        "mean_accuracy = sum(accuracy_dict.values()) / len(accuracy_dict)\n",
        "mean_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762f574d-6b1a-4573-8f02-d3a940688720",
        "id": "Si6p040G52H-"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated for computer_security_test.csv, accuracy: 0.0\n",
            "Calculated for security_studies_test.csv, accuracy: 0.020833333333333332\n",
            "Calculated for professional_medicine_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_biology_test.csv, accuracy: 0.01639344262295082\n",
            "Calculated for business_ethics_test.csv, accuracy: 0.0\n",
            "Calculated for nutrition_test.csv, accuracy: 0.01639344262295082\n",
            "Calculated for professional_psychology_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_statistics_test.csv, accuracy: 0.0\n",
            "Calculated for medical_genetics_test.csv, accuracy: 0.0\n",
            "Calculated for sociology_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_physics_test.csv, accuracy: 0.0\n",
            "Calculated for college_biology_test.csv, accuracy: 0.14285714285714285\n",
            "Calculated for high_school_government_and_politics_test.csv, accuracy: 0.0\n",
            "Calculated for jurisprudence_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_world_history_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_european_history_test.csv, accuracy: 0.0\n",
            "Calculated for international_law_test.csv, accuracy: 0.0\n",
            "Calculated for human_sexuality_test.csv, accuracy: 0.0\n",
            "Calculated for conceptual_physics_test.csv, accuracy: 0.08695652173913043\n",
            "Calculated for moral_scenarios_test.csv, accuracy: 0.011235955056179775\n",
            "Calculated for anatomy_test.csv, accuracy: 0.15384615384615385\n",
            "Calculated for elementary_mathematics_test.csv, accuracy: 0.08\n",
            "Calculated for human_aging_test.csv, accuracy: 0.045454545454545456\n",
            "Calculated for electrical_engineering_test.csv, accuracy: 0.03571428571428571\n",
            "Calculated for college_mathematics_test.csv, accuracy: 0.0\n",
            "Calculated for management_test.csv, accuracy: 0.1\n",
            "Calculated for high_school_us_history_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_mathematics_test.csv, accuracy: 0.0\n",
            "Calculated for professional_law_test.csv, accuracy: 0.0\n",
            "Calculated for clinical_knowledge_test.csv, accuracy: 0.038461538461538464\n",
            "Calculated for college_medicine_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_macroeconomics_test.csv, accuracy: 0.05194805194805195\n",
            "Calculated for global_facts_test.csv, accuracy: 0.21052631578947367\n",
            "Calculated for high_school_computer_science_test.csv, accuracy: 0.0\n",
            "Calculated for marketing_test.csv, accuracy: 0.021739130434782608\n",
            "Calculated for moral_disputes_test.csv, accuracy: 0.0\n",
            "Calculated for public_relations_test.csv, accuracy: 0.0\n",
            "Calculated for professional_accounting_test.csv, accuracy: 0.0\n",
            "Calculated for abstract_algebra_test.csv, accuracy: 0.05263157894736842\n",
            "Calculated for econometrics_test.csv, accuracy: 0.0\n",
            "Calculated for world_religions_test.csv, accuracy: 0.029411764705882353\n",
            "Calculated for high_school_chemistry_test.csv, accuracy: 0.025\n",
            "Calculated for astronomy_test.csv, accuracy: 0.0\n",
            "Calculated for machine_learning_test.csv, accuracy: 0.0\n",
            "Calculated for philosophy_test.csv, accuracy: 0.06451612903225806\n",
            "Calculated for high_school_microeconomics_test.csv, accuracy: 0.06382978723404255\n",
            "Calculated for college_computer_science_test.csv, accuracy: 0.0\n",
            "Calculated for logical_fallacies_test.csv, accuracy: 0.0\n",
            "Calculated for miscellaneous_test.csv, accuracy: 0.11538461538461539\n",
            "Calculated for formal_logic_test.csv, accuracy: 0.04\n",
            "Calculated for college_physics_test.csv, accuracy: 0.0\n",
            "Calculated for virology_test.csv, accuracy: 0.030303030303030304\n",
            "Calculated for high_school_psychology_test.csv, accuracy: 0.046296296296296294\n",
            "Calculated for college_chemistry_test.csv, accuracy: 0.0\n",
            "Calculated for us_foreign_policy_test.csv, accuracy: 0.05263157894736842\n",
            "Calculated for prehistory_test.csv, accuracy: 0.015625\n",
            "Calculated for high_school_geography_test.csv, accuracy: 0.07692307692307693\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02885811785358698"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_dict = count_acc_all_subjects(my_model, questions_dir, 5)\n",
        "mean_accuracy = sum(accuracy_dict.values()) / len(accuracy_dict)\n",
        "mean_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAdWss6J-RR_",
        "outputId": "4b262b18-c69e-4bfd-a0e5-e8257016e2ee"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated for computer_security_test.csv, accuracy: 0.0\n",
            "Calculated for security_studies_test.csv, accuracy: 0.0\n",
            "Calculated for professional_medicine_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_biology_test.csv, accuracy: 0.0\n",
            "Calculated for business_ethics_test.csv, accuracy: 0.0\n",
            "Calculated for nutrition_test.csv, accuracy: 0.0\n",
            "Calculated for professional_psychology_test.csv, accuracy: 0.01639344262295082\n",
            "Calculated for high_school_statistics_test.csv, accuracy: 0.0\n",
            "Calculated for medical_genetics_test.csv, accuracy: 0.15789473684210525\n",
            "Calculated for sociology_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_physics_test.csv, accuracy: 0.0\n",
            "Calculated for college_biology_test.csv, accuracy: 0.10714285714285714\n",
            "Calculated for high_school_government_and_politics_test.csv, accuracy: 0.0\n",
            "Calculated for jurisprudence_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_world_history_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_european_history_test.csv, accuracy: 0.0\n",
            "Calculated for international_law_test.csv, accuracy: 0.0\n",
            "Calculated for human_sexuality_test.csv, accuracy: 0.07692307692307693\n",
            "Calculated for conceptual_physics_test.csv, accuracy: 0.08695652173913043\n",
            "Calculated for moral_scenarios_test.csv, accuracy: 0.0\n",
            "Calculated for anatomy_test.csv, accuracy: 0.15384615384615385\n",
            "Calculated for elementary_mathematics_test.csv, accuracy: 0.06666666666666667\n",
            "Calculated for human_aging_test.csv, accuracy: 0.11363636363636363\n",
            "Calculated for electrical_engineering_test.csv, accuracy: 0.21428571428571427\n",
            "Calculated for college_mathematics_test.csv, accuracy: 0.0\n",
            "Calculated for management_test.csv, accuracy: 0.2\n",
            "Calculated for high_school_us_history_test.csv, accuracy: 0.025\n",
            "Calculated for high_school_mathematics_test.csv, accuracy: 0.018867924528301886\n",
            "Calculated for professional_law_test.csv, accuracy: 0.0032679738562091504\n",
            "Calculated for clinical_knowledge_test.csv, accuracy: 0.057692307692307696\n",
            "Calculated for college_medicine_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_macroeconomics_test.csv, accuracy: 0.0\n",
            "Calculated for global_facts_test.csv, accuracy: 0.21052631578947367\n",
            "Calculated for high_school_computer_science_test.csv, accuracy: 0.05263157894736842\n",
            "Calculated for marketing_test.csv, accuracy: 0.043478260869565216\n",
            "Calculated for moral_disputes_test.csv, accuracy: 0.0\n",
            "Calculated for public_relations_test.csv, accuracy: 0.09523809523809523\n",
            "Calculated for professional_accounting_test.csv, accuracy: 0.0\n",
            "Calculated for abstract_algebra_test.csv, accuracy: 0.05263157894736842\n",
            "Calculated for econometrics_test.csv, accuracy: 0.045454545454545456\n",
            "Calculated for world_religions_test.csv, accuracy: 0.11764705882352941\n",
            "Calculated for high_school_chemistry_test.csv, accuracy: 0.025\n",
            "Calculated for astronomy_test.csv, accuracy: 0.03333333333333333\n",
            "Calculated for machine_learning_test.csv, accuracy: 0.0\n",
            "Calculated for philosophy_test.csv, accuracy: 0.03225806451612903\n",
            "Calculated for high_school_microeconomics_test.csv, accuracy: 0.02127659574468085\n",
            "Calculated for college_computer_science_test.csv, accuracy: 0.0\n",
            "Calculated for logical_fallacies_test.csv, accuracy: 0.0625\n",
            "Calculated for miscellaneous_test.csv, accuracy: 0.08333333333333333\n",
            "Calculated for formal_logic_test.csv, accuracy: 0.0\n",
            "Calculated for college_physics_test.csv, accuracy: 0.0\n",
            "Calculated for virology_test.csv, accuracy: 0.09090909090909091\n",
            "Calculated for high_school_psychology_test.csv, accuracy: 0.018518518518518517\n",
            "Calculated for college_chemistry_test.csv, accuracy: 0.0\n",
            "Calculated for us_foreign_policy_test.csv, accuracy: 0.05263157894736842\n",
            "Calculated for prehistory_test.csv, accuracy: 0.0\n",
            "Calculated for high_school_geography_test.csv, accuracy: 0.07692307692307693\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04233096080837395"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('inherited_15000_gpt2-medium-10layers_mmlu_5_shot.json', 'w') as f:\n",
        "    json.dump(accuracy_dict, f)"
      ],
      "metadata": {
        "id": "wLlViPvE52H-"
      },
      "execution_count": 77,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}