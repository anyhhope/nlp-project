{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Данный проект основан на статье https://arxiv.org/abs/2404.08634. \\\n",
        "В статье описан метод inheritune, который заключается в том, чтобы взять первые n слоев готовой обученной модели и дообучить их.\\\n",
        "Цель этого метода – существенно сократить количество параметров модели и вследствие этого сэкономить ресурсы\\\n",
        "Основные пункты:\\\n",
        "1)\tБерется уже готовая модель, архитектура и веса которой обязательно известны (в нашем случае - GPT, в статье также рассматривалась Llama)\\\n",
        "2)\tМожно взять разное количество слоев от готовой модели. Авторы статьи брали от 33% до 50%\\\n",
        "3)\tВажно, что мы не добавляем новые слои и веса, а только оптимизируем уже имеющиеся\\\n",
        "4)\tДообучать модель нужно на том же датасете, на котором её учили изначально, соответственно это должна быть открытая информация.  \\\n",
        "5)\tДатасет берется не полностью, а лишь его часть\n",
        "\n"
      ],
      "metadata": {
        "id": "EbNhNM4Ln9ro"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1i9099AhDr9"
      },
      "source": [
        "# Gpt-2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для того, чтобы загрузить обученную модель, создаем класс GPT, в котором прописываем всю архитектуру модели."
      ],
      "metadata": {
        "id": "5MVQCsiohIup"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:44:38.894856Z",
          "iopub.status.busy": "2024-05-25T20:44:38.894352Z",
          "iopub.status.idle": "2024-05-25T20:44:40.657336Z",
          "shell.execute_reply": "2024-05-25T20:44:40.656336Z",
          "shell.execute_reply.started": "2024-05-25T20:44:38.894819Z"
        },
        "trusted": true,
        "id": "e9J54K3hhDsA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "nano-gpt implementation\n",
        "\n",
        "Full definition of a GPT Language Model, all of it in this single file.\n",
        "References:\n",
        "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
        "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
        "2) huggingface/transformers PyTorch implementation:\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
        "        # This behavior is deprecated and will be an error in future versions\"\n",
        "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # model surgery to decrease the block size if necessary\n",
        "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        # but want to use a smaller block size for some smaller, simpler model\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {} # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализируем модель уже обученными весами. В параметры передаем \"gpt2\". Всего есть четыре варианта - 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'. Выбираем самую маленькую, чтобы быстрее обучать её"
      ],
      "metadata": {
        "id": "39X5RSfQhWgT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:44:40.659567Z",
          "iopub.status.busy": "2024-05-25T20:44:40.659162Z",
          "iopub.status.idle": "2024-05-25T20:44:46.670640Z",
          "shell.execute_reply": "2024-05-25T20:44:46.669741Z",
          "shell.execute_reply.started": "2024-05-25T20:44:40.659540Z"
        },
        "trusted": true,
        "id": "Qf8O7_FKhDsD",
        "outputId": "05b15084-ee79-4a2b-d86f-b22376b44817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n"
          ]
        }
      ],
      "source": [
        "model = GPT.from_pretrained(\"gpt2\", dict(dropout=0.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:44:46.672452Z",
          "iopub.status.busy": "2024-05-25T20:44:46.671879Z",
          "iopub.status.idle": "2024-05-25T20:44:58.638755Z",
          "shell.execute_reply": "2024-05-25T20:44:58.637632Z",
          "shell.execute_reply.started": "2024-05-25T20:44:46.672415Z"
        },
        "trusted": true,
        "id": "6a8gz0vYhDsD",
        "outputId": "3dc33f6c-bb78-48b5-dcee-7602f4e97b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нам известно, какой токенайзер применялся при обучении данной модели, поэтому загружаем его же."
      ],
      "metadata": {
        "id": "2wKAT9UzhvK5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:44:58.640558Z",
          "iopub.status.busy": "2024-05-25T20:44:58.640227Z",
          "iopub.status.idle": "2024-05-25T20:44:59.034858Z",
          "shell.execute_reply": "2024-05-25T20:44:59.034047Z",
          "shell.execute_reply.started": "2024-05-25T20:44:58.640528Z"
        },
        "trusted": true,
        "id": "2xRBD963hDsE"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:44:59.038664Z",
          "iopub.status.busy": "2024-05-25T20:44:59.038014Z",
          "iopub.status.idle": "2024-05-25T20:44:59.095483Z",
          "shell.execute_reply": "2024-05-25T20:44:59.094422Z",
          "shell.execute_reply.started": "2024-05-25T20:44:59.038637Z"
        },
        "trusted": true,
        "id": "Hi58oIvhhDsE"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:44:59.112808Z",
          "iopub.status.busy": "2024-05-25T20:44:59.112539Z",
          "iopub.status.idle": "2024-05-25T20:44:59.124443Z",
          "shell.execute_reply": "2024-05-25T20:44:59.123522Z",
          "shell.execute_reply.started": "2024-05-25T20:44:59.112786Z"
        },
        "trusted": true,
        "collapsed": true,
        "id": "hEBn2C_ShDsE",
        "outputId": "9b76d28d-84aa-4272-c1d8-7f5bfdcffe49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:44:59.097070Z",
          "iopub.status.busy": "2024-05-25T20:44:59.096767Z",
          "iopub.status.idle": "2024-05-25T20:44:59.104707Z",
          "shell.execute_reply": "2024-05-25T20:44:59.103843Z",
          "shell.execute_reply.started": "2024-05-25T20:44:59.097045Z"
        },
        "trusted": true,
        "id": "Ifhk2k0NhDsE"
      },
      "outputs": [],
      "source": [
        "start_ids = encode(\"It is just a start\")\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=\"cpu\")[None, ...])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подаем в уже обученную модель какую-то фразу и смотрим, что она выдает связанный текст"
      ],
      "metadata": {
        "id": "YP7x9b6uh7Rb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:44:59.126146Z",
          "iopub.status.busy": "2024-05-25T20:44:59.125572Z",
          "iopub.status.idle": "2024-05-25T20:45:24.871079Z",
          "shell.execute_reply": "2024-05-25T20:45:24.870095Z",
          "shell.execute_reply.started": "2024-05-25T20:44:59.126115Z"
        },
        "trusted": true,
        "id": "3o4IVwiXhDsF",
        "outputId": "fc76e796-a7f5-4bac-f3fb-efcb66ec2e02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is just a start,\" said Massman. \"We have no girls now.\" So far 42 Aurora Valley Girls have been enrolled.\n",
            "\n",
            "Doctors have cured pregnant women with flip-flopping near the driver's side, closed the toilet often during hazing, and removed half or all of the pressure demons caused by smoking.\n",
            "\n",
            "\"If you want to now dial 99, we've dealt with a pregnant woman on birth control,\" said Dr. Dominic Trenicis. He pointed to the 64-year-old mother who has seen her children die three times since it began.\n",
            "\n",
            "Many fertility treatments that prevent half of the flare-ups among 1800 has been abandoned, said Trenicis.\n",
            "\n",
            "Dr. Josh Durand, another researcher of vasectomy attempts, said he, too, is skeptical.\n",
            "\n",
            "\"We see many dumps and spinning all the time,\" he said. One girl had a vasectomy and may have gestational diabetes. But about a third of deliveries go beyond that.\n",
            "\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    y = model.generate(x, 200)\n",
        "    print(decode(y[0].tolist()))\n",
        "    print('---------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем свой собственный класс, который наследует слои (в нашем случае 4) от готовой модели"
      ],
      "metadata": {
        "id": "dHnwUw0TiJ2l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:45:24.872760Z",
          "iopub.status.busy": "2024-05-25T20:45:24.872382Z",
          "iopub.status.idle": "2024-05-25T20:45:26.903098Z",
          "shell.execute_reply": "2024-05-25T20:45:26.902089Z",
          "shell.execute_reply.started": "2024-05-25T20:45:24.872726Z"
        },
        "trusted": true,
        "id": "-bND9JojhDsF",
        "outputId": "222a8322-3d95-497e-df36-d85d51819c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 66.95M\n"
          ]
        }
      ],
      "source": [
        "class MyGPT(GPT):\n",
        "    def __init__(self, config, base_model):\n",
        "        super().__init__(config)\n",
        "        # Копируем первые 4 слоев из base_model\n",
        "        self.transformer.h = nn.ModuleList([base_model.transformer.h[i] for i in range(4)])\n",
        "        self.transformer.wte = base_model.transformer.wte\n",
        "        self.transformer.wpe = base_model.transformer.wpe\n",
        "        self.transformer.drop = base_model.transformer.drop\n",
        "        self.transformer.ln_f = base_model.transformer.ln_f\n",
        "        self.lm_head = base_model.lm_head\n",
        "\n",
        "\n",
        "config = GPTConfig(\n",
        "    block_size=1024,\n",
        "    vocab_size=50257,\n",
        "    n_layer=4,\n",
        "    n_head=model.config.n_head,\n",
        "    n_embd=model.config.n_embd,\n",
        "    dropout=0.0,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "# Модель с 4 слоями\n",
        "my_model = MyGPT(config, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:45:26.904571Z",
          "iopub.status.busy": "2024-05-25T20:45:26.904254Z",
          "iopub.status.idle": "2024-05-25T20:45:26.911753Z",
          "shell.execute_reply": "2024-05-25T20:45:26.910736Z",
          "shell.execute_reply.started": "2024-05-25T20:45:26.904544Z"
        },
        "trusted": true,
        "collapsed": true,
        "id": "w-1yJUXuhDsF",
        "outputId": "488ae962-5cf8-4212-9774-1b7afb1da307"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MyGPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-3): 4 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:45:26.913223Z",
          "iopub.status.busy": "2024-05-25T20:45:26.912949Z",
          "iopub.status.idle": "2024-05-25T20:45:26.918302Z",
          "shell.execute_reply": "2024-05-25T20:45:26.917411Z",
          "shell.execute_reply.started": "2024-05-25T20:45:26.913199Z"
        },
        "trusted": true,
        "id": "-9k5E-sghDsF"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналогично передаем фразу уже в нашу собственную модель. Видно, что ей требуется дообучение"
      ],
      "metadata": {
        "id": "VwsBBD9yiXQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:45:26.919963Z",
          "iopub.status.busy": "2024-05-25T20:45:26.919602Z",
          "iopub.status.idle": "2024-05-25T20:45:36.339991Z",
          "shell.execute_reply": "2024-05-25T20:45:36.339018Z",
          "shell.execute_reply.started": "2024-05-25T20:45:26.919933Z"
        },
        "trusted": true,
        "id": "8l6DWQ7thDsG",
        "outputId": "c8d102ca-10e4-495a-cd06-90fe72d2a129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is just a start heretic acid acid acid acid acid acid acid acid acid tankage 47308565656431(2) Extended Edition Edition Edition 4345308100656060393978805. Sincerelyricsricsricsricsinger-style style currieceeqiao...BUTtonstons/etcetcetcetcetcetcetereterpentium strike exact same exact Same exact same thing happened hereaboutsaboutsaboutsaboutsaboutsaboutsaboutsabouts\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------- Times outburgerger-style variant variant variants variants variants variants variants variants variants variants variants variants paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths paths path paths paths paths paths paths path paths towards toward toward the same amount of workstationmarkmarking dot dot dot dot dot dotningningningningningningningningningningningningningningningningningningningningningningningningningningningningningning giftme: 0x2\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "my_model.eval()\n",
        "with torch.no_grad():\n",
        "    y = my_model.generate(x, 200)\n",
        "    print(decode(y[0].tolist()))\n",
        "    print('---------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3G43aSghDsG"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно взять целый датасет для дообучения, но мы возьмем только часть, для экономии ресурсов"
      ],
      "metadata": {
        "id": "OMCx0O9PjH2k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:45:36.344880Z",
          "iopub.status.busy": "2024-05-25T20:45:36.344521Z",
          "iopub.status.idle": "2024-05-25T20:45:36.349829Z",
          "shell.execute_reply": "2024-05-25T20:45:36.348910Z",
          "shell.execute_reply.started": "2024-05-25T20:45:36.344853Z"
        },
        "trusted": true,
        "id": "0qE4f_sOhDsG"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset(\"Skylion007/openwebtext\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем часть ТОГО ЖЕ датасета, на котором изначально обучалась модель"
      ],
      "metadata": {
        "id": "z0c8OwNJiiVs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:45:36.351462Z",
          "iopub.status.busy": "2024-05-25T20:45:36.350947Z",
          "iopub.status.idle": "2024-05-25T20:45:37.576435Z",
          "shell.execute_reply": "2024-05-25T20:45:37.575223Z",
          "shell.execute_reply.started": "2024-05-25T20:45:36.351429Z"
        },
        "trusted": true,
        "id": "vcloiXGNhDsG",
        "outputId": "14e65d95-7fe3-42a6-8dc9-b7f6e81bb901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-05-25 20:45:37--  https://raw.githubusercontent.com/anyhhope/sample-df/main/combined_text_short.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10086822 (9.6M) [text/plain]\n",
            "Saving to: 'combined_text_short.txt'\n",
            "\n",
            "combined_text_short 100%[===================>]   9.62M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-05-25 20:45:37 (100 MB/s) - 'combined_text_short.txt' saved [10086822/10086822]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !wget https://raw.githubusercontent.com/anyhhope/sample-df/main/combined_text_short.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:45:37.578915Z",
          "iopub.status.busy": "2024-05-25T20:45:37.578049Z",
          "iopub.status.idle": "2024-05-25T20:45:49.261532Z",
          "shell.execute_reply": "2024-05-25T20:45:49.260704Z",
          "shell.execute_reply.started": "2024-05-25T20:45:37.578883Z"
        },
        "trusted": true,
        "id": "uRMqqhcmhDsG"
      },
      "outputs": [],
      "source": [
        "with open('/kaggle/input/opentextmlrd/combined_text_mlrd.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делим данные на train, val"
      ],
      "metadata": {
        "id": "oO5f5f4PitIG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:49:33.862042Z",
          "iopub.status.busy": "2024-05-25T20:49:33.861104Z",
          "iopub.status.idle": "2024-05-25T20:53:33.151093Z",
          "shell.execute_reply": "2024-05-25T20:53:33.149973Z",
          "shell.execute_reply.started": "2024-05-25T20:49:33.862003Z"
        },
        "trusted": true,
        "id": "zxbHTfb4hDsG"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задаем параметры модели"
      ],
      "metadata": {
        "id": "WJDWnnSli06T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:53:50.554937Z",
          "iopub.status.busy": "2024-05-25T20:53:50.554183Z",
          "iopub.status.idle": "2024-05-25T20:53:50.563446Z",
          "shell.execute_reply": "2024-05-25T20:53:50.562551Z",
          "shell.execute_reply.started": "2024-05-25T20:53:50.554904Z"
        },
        "trusted": true,
        "id": "Pyka8xMIhDsH",
        "outputId": "931b0cfa-26ee-4120-b6fb-5ff26394edce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x798630189eb0>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 256\n",
        "max_iters = 100000\n",
        "eval_interval = 100\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 10\n",
        "save_interval = 1000 # должно быть кратно eval_interval\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(985)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:53:52.338496Z",
          "iopub.status.busy": "2024-05-25T20:53:52.337714Z",
          "iopub.status.idle": "2024-05-25T20:53:52.565005Z",
          "shell.execute_reply": "2024-05-25T20:53:52.564106Z",
          "shell.execute_reply.started": "2024-05-25T20:53:52.338463Z"
        },
        "trusted": true,
        "collapsed": true,
        "id": "8UQLPGS1hDsH",
        "outputId": "ccf069f0-dd8a-4190-fcce-9c8744d46491"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MyGPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-3): 4 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:53:54.936569Z",
          "iopub.status.busy": "2024-05-25T20:53:54.936194Z",
          "iopub.status.idle": "2024-05-25T20:53:54.943546Z",
          "shell.execute_reply": "2024-05-25T20:53:54.942538Z",
          "shell.execute_reply.started": "2024-05-25T20:53:54.936540Z"
        },
        "trusted": true,
        "id": "Iamc_c8QhDsH"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    my_model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = my_model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "            print(loss.item())\n",
        "            torch.cuda.empty_cache()\n",
        "        out[split] = losses.mean()\n",
        "    my_model.train()\n",
        "    del X\n",
        "    del Y\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запускаем функцию обучения"
      ],
      "metadata": {
        "id": "WOWKClNJi4xR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:54:29.132338Z",
          "iopub.status.busy": "2024-05-25T20:54:29.131447Z",
          "iopub.status.idle": "2024-05-25T20:56:46.340628Z",
          "shell.execute_reply": "2024-05-25T20:56:46.339237Z",
          "shell.execute_reply.started": "2024-05-25T20:54:29.132303Z"
        },
        "trusted": true,
        "id": "SFdpxvfxhDsH",
        "outputId": "443bd8ee-a173-471c-e987-a6c7faab12fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter: 0\n",
            "8.783082008361816\n",
            "8.831452369689941\n",
            "8.887443542480469\n",
            "8.764090538024902\n",
            "8.741375923156738\n",
            "8.791605949401855\n",
            "8.632545471191406\n",
            "8.703487396240234\n",
            "8.85702133178711\n",
            "8.71181583404541\n",
            "8.741567611694336\n",
            "8.5383939743042\n",
            "8.73711109161377\n",
            "8.627869606018066\n",
            "8.697270393371582\n",
            "8.682490348815918\n",
            "8.78878116607666\n",
            "8.865164756774902\n",
            "8.753304481506348\n",
            "8.766968727111816\n",
            "step 0: train loss 8.7704, val loss 8.7199\n",
            "Iter: 1\n",
            "Iter: 2\n",
            "Iter: 3\n",
            "Iter: 4\n",
            "Iter: 5\n",
            "Iter: 6\n",
            "Iter: 7\n",
            "Iter: 8\n",
            "Iter: 9\n",
            "Iter: 10\n",
            "5.474310398101807\n",
            "5.459348201751709\n",
            "5.501533031463623\n",
            "5.521154403686523\n",
            "5.598445892333984\n",
            "5.464452266693115\n",
            "5.583437442779541\n",
            "5.532219409942627\n",
            "5.672239303588867\n",
            "5.580300331115723\n",
            "5.42786169052124\n",
            "5.4406232833862305\n",
            "5.455196857452393\n",
            "5.442969799041748\n",
            "5.536002159118652\n",
            "5.664560317993164\n",
            "5.581079006195068\n",
            "5.539344787597656\n",
            "5.602444648742676\n",
            "5.644266128540039\n",
            "step 10: train loss 5.5387, val loss 5.5334\n",
            "Iter: 11\n",
            "Iter: 12\n",
            "Iter: 13\n",
            "Iter: 14\n",
            "Iter: 15\n",
            "Iter: 16\n",
            "Iter: 17\n",
            "Iter: 18\n",
            "Iter: 19\n",
            "Iter: 20\n",
            "5.37289571762085\n",
            "5.192713260650635\n",
            "5.307033538818359\n",
            "5.241941928863525\n",
            "5.30610466003418\n",
            "5.383307456970215\n",
            "5.226146221160889\n",
            "5.271598815917969\n",
            "5.241030693054199\n",
            "5.217411518096924\n",
            "5.294187545776367\n",
            "5.310798645019531\n",
            "5.325101852416992\n",
            "5.333522319793701\n",
            "5.3249077796936035\n",
            "5.364990234375\n",
            "5.264810562133789\n",
            "5.335384845733643\n",
            "5.364226818084717\n",
            "5.364969730377197\n",
            "step 20: train loss 5.2760, val loss 5.3283\n",
            "Iter: 21\n",
            "Iter: 22\n",
            "Iter: 23\n",
            "Iter: 24\n",
            "Iter: 25\n",
            "Iter: 26\n",
            "Iter: 27\n",
            "Iter: 28\n",
            "Iter: 29\n",
            "Iter: 30\n",
            "5.3533453941345215\n",
            "5.323523044586182\n",
            "5.174818992614746\n",
            "5.232402324676514\n",
            "5.124032974243164\n",
            "5.259528160095215\n",
            "5.193236351013184\n",
            "5.153775691986084\n",
            "5.178406238555908\n",
            "5.304696083068848\n",
            "5.230146408081055\n",
            "5.1774582862854\n",
            "5.323805809020996\n",
            "5.273484706878662\n",
            "5.094062328338623\n",
            "5.2527594566345215\n",
            "5.162446022033691\n",
            "5.1948018074035645\n",
            "5.218825817108154\n",
            "5.191963195800781\n",
            "step 30: train loss 5.2298, val loss 5.2120\n",
            "Iter: 31\n",
            "Iter: 32\n",
            "Iter: 33\n",
            "Iter: 34\n",
            "Iter: 35\n",
            "Iter: 36\n",
            "Iter: 37\n",
            "Iter: 38\n",
            "Iter: 39\n",
            "Iter: 40\n",
            "5.214907646179199\n",
            "5.283825397491455\n",
            "5.102501392364502\n",
            "5.172941207885742\n",
            "5.167786121368408\n",
            "5.248371124267578\n",
            "5.1098198890686035\n",
            "5.181862831115723\n",
            "5.0895771980285645\n",
            "5.091322422027588\n",
            "5.049291610717773\n",
            "5.147934913635254\n",
            "5.090610027313232\n",
            "5.195910930633545\n",
            "5.310455322265625\n",
            "5.152013778686523\n",
            "5.175025939941406\n",
            "5.09750509262085\n",
            "5.158442974090576\n",
            "5.297215461730957\n",
            "step 40: train loss 5.1663, val loss 5.1674\n",
            "Iter: 41\n",
            "Iter: 42\n",
            "Iter: 43\n",
            "Iter: 44\n",
            "Iter: 45\n",
            "Iter: 46\n",
            "Iter: 47\n",
            "Iter: 48\n",
            "Iter: 49\n",
            "Iter: 50\n",
            "4.962555885314941\n",
            "5.063338279724121\n",
            "5.070313453674316\n",
            "4.987173080444336\n",
            "5.002044677734375\n",
            "5.203602313995361\n",
            "5.17853307723999\n",
            "5.165411472320557\n",
            "5.067447185516357\n",
            "5.12283182144165\n",
            "5.134243965148926\n",
            "5.153112888336182\n",
            "5.07501745223999\n",
            "5.160361289978027\n",
            "5.144680976867676\n",
            "5.058602809906006\n",
            "5.0686774253845215\n",
            "5.011592388153076\n",
            "5.043570518493652\n",
            "5.125550270080566\n",
            "step 50: train loss 5.0823, val loss 5.0975\n",
            "Iter: 51\n",
            "Iter: 52\n",
            "Iter: 53\n",
            "Iter: 54\n",
            "Iter: 55\n",
            "Iter: 56\n",
            "Iter: 57\n",
            "Iter: 58\n",
            "Iter: 59\n",
            "Iter: 60\n",
            "5.005153656005859\n",
            "5.146929740905762\n",
            "4.9642767906188965\n",
            "5.104104518890381\n",
            "4.928366661071777\n",
            "4.999648094177246\n",
            "4.947485446929932\n",
            "5.050122261047363\n",
            "5.054410457611084\n",
            "5.046616554260254\n",
            "5.032743453979492\n",
            "5.21943998336792\n",
            "4.912429332733154\n",
            "5.062443733215332\n",
            "5.243010997772217\n",
            "5.188401222229004\n",
            "5.024115562438965\n",
            "4.973352909088135\n",
            "4.9360032081604\n",
            "5.020114898681641\n",
            "step 60: train loss 5.0247, val loss 5.0612\n",
            "Iter: 61\n",
            "Iter: 62\n",
            "Iter: 63\n",
            "Iter: 64\n",
            "Iter: 65\n",
            "Iter: 66\n",
            "Iter: 67\n",
            "Iter: 68\n",
            "Iter: 69\n",
            "Iter: 70\n",
            "5.012983798980713\n",
            "5.055940628051758\n",
            "4.904860973358154\n",
            "4.935322284698486\n",
            "4.964346408843994\n",
            "5.020248889923096\n",
            "4.9315900802612305\n",
            "5.022463321685791\n",
            "5.002845764160156\n",
            "4.941353797912598\n",
            "5.017144203186035\n",
            "5.0985260009765625\n",
            "5.093120574951172\n",
            "4.874557971954346\n",
            "5.047169208526611\n",
            "5.057168483734131\n",
            "4.820476531982422\n",
            "4.96372652053833\n",
            "5.019594669342041\n",
            "5.038033962249756\n",
            "step 70: train loss 4.9792, val loss 5.0030\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Сохранение модели и оптимизатора\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: my_model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m     }\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint_iter_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m my_model(xb, yb)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 619\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:853\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    852\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 853\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(my_model.parameters(), lr=learning_rate)\n",
        "losses_train = {}\n",
        "losses_val = {}\n",
        "for iter in range(max_iters):\n",
        "    print(f\"Iter: {iter}\")\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        losses_train[iter] = losses['train']\n",
        "        losses_val[iter] = losses['val']\n",
        "\n",
        "        if iter % save_interval == 0:\n",
        "            # Сохранение модели и оптимизатора\n",
        "            checkpoint = {\n",
        "                'model_state_dict': my_model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'iter': iter,\n",
        "                'train_loss': losses['train'],\n",
        "                'val_loss': losses['val']\n",
        "            }\n",
        "            torch.save(checkpoint, f\"checkpoint_iter_{iter}.pt\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = my_model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    del xb, yb, logits, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:59:57.385634Z",
          "iopub.status.busy": "2024-05-25T20:59:57.385240Z",
          "iopub.status.idle": "2024-05-25T20:59:58.224042Z",
          "shell.execute_reply": "2024-05-25T20:59:58.223033Z",
          "shell.execute_reply.started": "2024-05-25T20:59:57.385596Z"
        },
        "trusted": true,
        "id": "H6tA-Y8ehDsH",
        "outputId": "18998547-28a8-4380-d959-e0257db7fd11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is just a start from a new library. It did work on all applications. We always wanted to improve infrastructure hard Pay Now service linker, braking cars, aren't garded with such a secure browser, anymore—or it is in the way to say the LCL was available. And the system generated a path to JavaScript, so that once learning, servers work better for those kind of continuous changes. you want to treat to no matter any backup scenario, using hyperflexibles easier to imagine creating a dynamic or backend using the application, minimize the bookies that you struggle, including you lose. If you have something to notice that there was more of the API help from the underlying medical decision as intent of passion.\n",
            "\n",
            "Bufferences: Three included torrential system balances across media before 25 payment’supoption is made. This concept is notbinding because a human element preventing which we are using services.\n",
            "\n",
            "Quasorpurit-specific list is found during certain times it wasn't\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "start_ids = encode(\"It is just a start\")\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "my_model.eval()\n",
        "with torch.no_grad():\n",
        "    y = my_model.generate(x, 200)\n",
        "    print(decode(y[0].tolist()))\n",
        "    print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-25T20:58:07.966607Z",
          "iopub.status.busy": "2024-05-25T20:58:07.966097Z",
          "iopub.status.idle": "2024-05-25T20:58:08.476225Z",
          "shell.execute_reply": "2024-05-25T20:58:08.475310Z",
          "shell.execute_reply.started": "2024-05-25T20:58:07.966572Z"
        },
        "trusted": true,
        "id": "tR3cOV6dhDsH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(losses_val.keys(), losses_val.values(), marker='o', linestyle='-')\n",
        "plt.plot(losses_val.keys(), losses_train.values(), marker='o', linestyle='-')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Losses')\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 5084933,
          "sourceId": 8516895,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30698,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}